{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Financial Offer Propensity — From Exploration to Production\n",
    "\n",
    "This notebook simulates the **exploratory phase** of the project: data analysis, model training, and evaluation. The rest of the codebase (pipelines, serving, Streamlit app, Docker, monitoring) was then created to **productionize** this prototype.\n",
    "\n",
    "**Flow:** Load UCI Bank Marketing data → EDA → Feature engineering → Train models → Evaluate → (Production pipeline mirrors this in `src/pipelines/`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and load data\n",
    "\n",
    "We use the UCI Bank Marketing dataset. Run `make data` from the repo root first, or the path below will need to point to where the CSV lives."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Run from repo root so imports work\n",
    "ROOT = Path(os.path.abspath(\"\")).resolve()\n",
    "if str(ROOT.name) == \"notebooks\":\n",
    "    ROOT = ROOT.parent\n",
    "os.chdir(ROOT)\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "RAW_PATH = ROOT / \"data\" / \"raw\" / \"bank-additional-full.csv\"\n",
    "if not RAW_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Data not found at {RAW_PATH}. Run 'make data' from repo root.\")\n",
    "\n",
    "df = pd.read_csv(RAW_PATH, sep=\";\", encoding=\"utf-8\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis (EDA)\n",
    "\n",
    "Understand schema, target balance, and key distributions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Schema and dtypes\n",
    "print(\"Columns and dtypes:\")\n",
    "df.dtypes"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Target: subscription (yes/no)\n",
    "target = \"y\"\n",
    "print(\"Target distribution:\")\n",
    "print(df[target].value_counts())\n",
    "print(f\"\\nAcceptance rate: {df[target].eq('yes').mean():.2%}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Numerical summary\n",
    "num_cols = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "num_cols = [c for c in num_cols if c in df.columns]\n",
    "df[num_cols].describe()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Categorical value counts (sample)\n",
    "cat_cols = [\"job\", \"marital\", \"education\", \"contact\", \"poutcome\"]\n",
    "for c in cat_cols:\n",
    "    if c in df.columns:\n",
    "        print(f\"\\n{c}:\")\n",
    "        print(df[c].value_counts().head(8))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Visual: age and balance by target\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "df[\"y_bin\"] = (df[\"y\"] == \"yes\").astype(int)\n",
    "df.boxplot(column=\"age\", by=\"y\", ax=axes[0])\n",
    "axes[0].set_title(\"Age by subscription\")\n",
    "df.boxplot(column=\"balance\", by=\"y\", ax=axes[1])\n",
    "axes[1].set_title(\"Balance by subscription\")\n",
    "plt.suptitle(\"\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature engineering\n",
    "\n",
    "Same choices as in production: numerical columns scaled, categorical one-hot encoded (drop first)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "NUM_COLS = [\"age\", \"balance\", \"day\", \"duration\", \"campaign\", \"pdays\", \"previous\"]\n",
    "CAT_COLS = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\n",
    "NUM_COLS = [c for c in NUM_COLS if c in df.columns]\n",
    "CAT_COLS = [c for c in CAT_COLS if c in df.columns]\n",
    "\n",
    "y = (df[\"y\"].astype(str).str.lower() == \"yes\").astype(int).values\n",
    "preprocessor = ColumnTransformer(\n",
    "    [\n",
    "        (\"num\", StandardScaler(), NUM_COLS),\n",
    "        (\"cat\", OneHotEncoder(drop=\"first\", handle_unknown=\"ignore\"), CAT_COLS),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "X = df[NUM_COLS + CAT_COLS]\n",
    "X_enc = preprocessor.fit_transform(X)\n",
    "feature_names = NUM_COLS + list(preprocessor.named_transformers_[\"cat\"].get_feature_names_out(CAT_COLS))\n",
    "print(f\"Features: {X_enc.shape[1]}, samples: {X_enc.shape[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "RANDOM_STATE = 42\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_enc, y, train_size=0.8, random_state=RANDOM_STATE, stratify=y)\n",
    "print(f\"Train: {X_train.shape[0]}, Test: {X_test.shape[0]}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model training\n",
    "\n",
    "We train a **Logistic Regression** (baseline) and **Gradient Boosting** (primary), same as in `src/pipelines/train.py`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "lr = LogisticRegression(C=1.0, max_iter=1000, solver=\"lbfgs\", random_state=RANDOM_STATE)\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Logistic Regression — train accuracy:\", lr.score(X_train, y_train).round(4))\n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, max_depth=5, learning_rate=0.1, min_samples_leaf=10, random_state=RANDOM_STATE)\n",
    "gb.fit(X_train, y_train)\n",
    "print(\"Gradient Boosting — train accuracy:\", gb.score(X_train, y_train).round(4))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "ROC-AUC, precision, recall, F1, confusion matrix, and calibration (same metrics as `src/pipelines/evaluate.py`)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    average_precision_score,\n",
    "    precision_recall_fscore_support,\n",
    "    accuracy_score,\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    ")\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "model = gb  # primary model\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"--- Test set metrics ---\")\n",
    "print(f\"ROC-AUC:           {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(f\"Average precision: {average_precision_score(y_test, y_prob):.4f}\")\n",
    "print(f\"Accuracy:          {accuracy_score(y_test, y_pred):.4f}\")\n",
    "prec, rec, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", zero_division=0)\n",
    "print(f\"Precision:         {prec:.4f}\")\n",
    "print(f\"Recall:            {rec:.4f}\")\n",
    "print(f\"F1:                {f1:.4f}\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)\n",
    "cal_mae = np.abs(prob_true - prob_pred).mean()\n",
    "print(f\"Calibration MAE:   {cal_mae:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(5, 4))\n",
    "plt.plot(prob_pred, prob_true, \"s-\", label=\"Model\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect\")\n",
    "plt.xlabel(\"Mean predicted probability\")\n",
    "plt.ylabel(\"Fraction of positives\")\n",
    "plt.title(\"Calibration curve\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No\", \"Yes\"])\n",
    "disp.plot(cmap=\"Blues\")\n",
    "plt.title(\"Confusion matrix (test set)\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. From prototype to production\n",
    "\n",
    "This exploratory workflow was **productionized** into:\n",
    "\n",
    "- **`src/pipelines/ingest.py`** — load and validate raw data\n",
    "- **`src/pipelines/features.py`** — same preprocessor (ColumnTransformer + feature names)\n",
    "- **`src/pipelines/train.py`** — same models and train/test split\n",
    "- **`src/pipelines/evaluate.py`** — same metrics and calibration\n",
    "- **`src/serving/predict.py`** — load saved model + preprocessor for inference\n",
    "- **`src/app/streamlit_app.py`** — UI that calls the same inference for propensity and ranking\n",
    "\n",
    "Run `make train` and `make run` (or the Docker image) to use the production pipeline and app."
   ]
  }
 ]
}
